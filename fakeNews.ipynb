{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to read the data from a fake news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    House Dem Aide: We Didn’t Even See Comey’s Let...\n",
       "1    FLYNN: Hillary Clinton, Big Woman on Campus - ...\n",
       "2                    Why the Truth Might Get You Fired\n",
       "3    15 Civilians Killed In Single US Airstrike Hav...\n",
       "4    Iranian woman jailed for fictional unpublished...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('train.csv')\n",
    "x=data['title']\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, after seperating out the target column which identifies the articles in the dataset as Fake or Not Fake. It is named as the label variable and attributed to y whereas the headline is attributed to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=data['label']\n",
    "print(y.shape)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify this code Raksha...make this to output using only one line. ..well here it's just counting the number of 1s and 0s in the dataset. It would perform bettr only if the values corresponding to the two are as close as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones : 10413\n",
      "zeros : 10387\n"
     ]
    }
   ],
   "source": [
    "ones=0\n",
    "zeros=0\n",
    "for i in y:\n",
    "    if i==1:\n",
    "        ones+=1\n",
    "    else:\n",
    "        zeros+=1\n",
    "print(\"ones :\",ones)\n",
    "print(\"zeros :\",zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all the news headlines are bought in a list format to help in further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It',\n",
       " 'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart',\n",
       " 'Why the Truth Might Get You Fired',\n",
       " '15 Civilians Killed In Single US Airstrike Have Been Identified',\n",
       " 'Iranian woman jailed for fictional unpublished story about woman stoned to death for adultery']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[]\n",
    "for i in x:\n",
    "    sentences.append(str(i))\n",
    "print(len(sentences))\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the Dataset for our purpose is prepared and split into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size=20000\n",
    "train_s = sentences[0:training_size]\n",
    "test_s = sentences[training_size:]\n",
    "train_labels = y[0:training_size]\n",
    "test_labels = y[training_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the interesting stuff happens, where we are padding to make all the lengths of the sentences equal and tokenize the text where we seperate the words out and also assign tokens aka numbers to each word available in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_s)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_s)\n",
    "\n",
    "train_padded = pad_sequences(train_sequences,maxlen=100,padding='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_s)\n",
    "\n",
    "test_padded = pad_sequences(test_sequences,maxlen=100,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tensorboard=TensorBoard(log_dir='logs/',profile_batch=10000000)\n",
    "vocab_size=100000\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,16,input_length=100),\n",
    "    #tf.keras.layers.LSTM(20),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 800 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 34s 2ms/sample - loss: 0.5835 - accuracy: 0.7297 - val_loss: 0.3689 - val_accuracy: 0.8675\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 32s 2ms/sample - loss: 0.2894 - accuracy: 0.8922 - val_loss: 0.2489 - val_accuracy: 0.8975\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 32s 2ms/sample - loss: 0.2309 - accuracy: 0.9063 - val_loss: 0.2394 - val_accuracy: 0.9038\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 29s 1ms/sample - loss: 0.2165 - accuracy: 0.9112 - val_loss: 0.2279 - val_accuracy: 0.9100\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 30s 1ms/sample - loss: 0.2121 - accuracy: 0.9118 - val_loss: 0.2379 - val_accuracy: 0.9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13191ef2b70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels),callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the neural netowrk model. when u o this a file named raxxy.h5 and raxxy.json would be created in  the directory where you are running this code...don't be scared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json=model.to_json()\n",
    "with open(\"raxxy.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save(\"raxxy.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake news\n"
     ]
    }
   ],
   "source": [
    "new_news = [\"granny starting to fear spiders in the garden might be real\"]\n",
    "sequences = tokenizer.texts_to_sequences(new_news)\n",
    "padded = pad_sequences(sequences, maxlen=100, padding='post')\n",
    "print(\"fake news\" if model.predict(padded) >= 0.5 else \"true shit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieving the model and asking for the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71901804]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model(\"raxxy.h5\")\n",
    "prediction=model.predict(padded)\n",
    "print(prediction)\n",
    "#do the above same thing in case you want lazy Raksha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
